{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11904043"
   },
   "source": [
    "# Enhance BART with a TF Computation Graph\n",
    "\n",
    "**Authors**: [Guanxiong Luo](mailto:guanxiong.luo@med.uni-goettingen.de), [Nick Scholand](mailto:nick.scholand@med.uni-goettingen.de), [Christian Holme](mailto:christian.holme@med.uni-goettingen.de)\n",
    "\n",
    "**Presenter**: [Guanxiong Luo](mailto:guanxiong.luo@med.uni-goettingen.de)\n",
    "\n",
    "**Institution**: University Medical Center Göttingen\n",
    "\n",
    "**Reference**:\n",
    "> Luo, G, Blumenthal, M, Uecker, M. Using data-driven image priors for image reconstruction with BART Proc. Intl. Soc. Mag. Reson. Med. 29 (2021) P.1756\n",
    "\n",
    "## Overview\n",
    "This tutorial is to present how to create a regularization term with tensorflow and use it for image reconstruction in [BART](https://github.com/mrirecon/bart).\n",
    "\n",
    "<img src=\"over.png\" width=\"800\"/>\n",
    "\n",
    "## What we have\n",
    "TensorFlow provides a C API that can be used to build bindings for other languages. \n",
    "\n",
    "1. BART src/nn/tf_wrapper.c\n",
    "\n",
    "    * create tensors, create tf session\n",
    "\n",
    "    * import the exported graph\n",
    "\n",
    "    * restore the session from the saved model\n",
    "\n",
    "    * get operation nodes from the graph\n",
    "\n",
    "    * execute operation with session.run()\n",
    "\n",
    "\n",
    "2. TensorFlow C Libraries [2.4.0](https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-2.4.0.tar.gz)\n",
    "\n",
    "3. A python program to export graph and weights (if any)\n",
    "\n",
    "## What you can do with tf graph\n",
    "\n",
    "> We can create the regularization term $R(x)$ with tf graph for image reconstruction (integrated in BART's `pics` tool).\n",
    "\n",
    "$$\\underset{x}{\\arg \\min}\\ \\|Ax-y\\|^2+\\lambda R(x)$$\n",
    "\n",
    "## What you can learn here\n",
    "\n",
    "1. simple example $R(x)=\\|x\\|^2$ without trainable weights\n",
    "\n",
    "2. $R(x)=\\log p(x, net(\\Theta,x))$ with trainable weights $\\Theta$, $net$ is represented as a prior [1]\n",
    "\n",
    "[1] Luo, G, Zhao, N, Jiang, W, Hui, ES, Cao, P. MRI reconstruction using deep Bayesian estimation. Magn Reson Med. 2020; 84: 2246– 2261. https://doi.org/10.1002/mrm.28274"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Download Supporting Material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial requires additional data including radial k-space spokes, a trained model and some python functions. If you want to follow up this tutorial execute the following cell, which downloads the required files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the required supporting materials\n",
    "! wget -q https://raw.githubusercontent.com/mrirecon/bart-workshop/master/ismrm2021/bart_tensorflow/data.zip\n",
    "! unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b544b17"
   },
   "source": [
    "## Part I: How to Create a TF Graph for BART\n",
    "\n",
    "The first part of this tutorial is about creating a TF graph, which can be used with BART. Therefore, we need load some python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "d9a21c71",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30b64c57"
   },
   "source": [
    "### Step 1: Define Input $x$\n",
    "\n",
    "We define a TF object for our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4d77905"
   },
   "outputs": [],
   "source": [
    "image_shape = [256, 256, 2]\n",
    "batch_size = 1\n",
    "\n",
    "# CAPI -> TF_GraphOperationByName(graph, \"input_0\")\n",
    "# give name with input_0, ..., input_I \n",
    "x = tf.placeholder(tf.float32,\n",
    "                   shape=[batch_size]+image_shape,\n",
    "                   name='input_0')\n",
    "v = tf.Variable(1.)\n",
    "x = x * v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e2719ae"
   },
   "source": [
    "### Step 2: Define Output $R(x)=\\|x\\|^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2920129"
   },
   "outputs": [],
   "source": [
    "l2 = tf.nn.l2_loss(x)#/np.product(image_shape)/batch_size        #R(x)=|x|^2\n",
    "# CAPI -> TF_GraphOperationByName(graph, \"output_0\") -> nlop forward\n",
    "# give name with output_0, ..., output_I\n",
    "output = tf.identity(tf.stack([l2, tf.ones_like(l2)], axis=-1), name='output_0') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e8f00fa"
   },
   "source": [
    "### Step 3: Define the Gradient of $R(x)=\\|x\\|^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d24fc05"
   },
   "outputs": [],
   "source": [
    "grad_ys = tf.placeholder(tf.float32,\n",
    "                         shape=[2],\n",
    "                         name='grad_ys_0')\n",
    "\n",
    "# CAPI -> TF_GraphOperationByName(graph, \"grad_0\") -> nlop adj\n",
    "grads = tf.squeeze(tf.gradients(output, x, grad_ys), name='grad_0') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88c848b3"
   },
   "source": [
    "### Step 4: Export the Graph and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0acc0c0",
    "outputId": "097bcdc1-597e-44d1-dd6c-3f72af686631"
   },
   "outputs": [],
   "source": [
    "from utils import export_model\n",
    "# export_model(model_path, exported_path, name, as_text, use_gpu):\n",
    "\n",
    "export_model(None, \"./\", \"l2_toy\", as_text=False, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49f7dc3f",
    "outputId": "e1ba81ad-466e-4025-cc2d-b1bd909b1052"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce4a7796"
   },
   "source": [
    "##  Part II: How to Use the TF Graph in BART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1bfbc28"
   },
   "source": [
    "###  Step 1: Setup BART and TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF C API\n",
    "\n",
    "First we need to **download the TF C API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ea147d2",
    "outputId": "b9282bf9-f5cb-4cdd-d582-28f43aeafc5b"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download tensorflow c libraries\n",
    "wget -q https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-2.4.0.tar.gz\n",
    "mkdir tensorflow && tar -C tensorflow -xvzf libtensorflow-gpu-linux-x86_64-2.4.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and set the required environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cOk227HH3Vv",
    "outputId": "7a1d4b1c-d32b-4bbe-ed32-707b7e8f6c5b"
   },
   "outputs": [],
   "source": [
    "%env LIBRARY_PATH=/content/tensorflow/include \n",
    "%env LD_LIBRARY_PATH=/content/tensorflow/lib\n",
    "%env TF_CPP_MIN_LOG_LEVEL=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Compile BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Install BARTs dependencies\n",
    "apt-get install -y make gcc libfftw3-dev liblapacke-dev libpng-dev libopenblas-dev &> /dev/null\n",
    "\n",
    "# Download BART version\n",
    "\n",
    "[ -d /content/bart ] && rm -r /content/bart\n",
    "git clone https://github.com/mrirecon/bart/ bart\n",
    "\n",
    "[ -d \"bart\" ] && echo \"BART branch ${BRANCH} was downloaded successfully.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading BART we need to compile it. Make sure the flags\n",
    "\n",
    "- `TENSORFLOW=1`\n",
    "- `TENSORFLOW_BASE=../tensorflow/`,\n",
    "\n",
    "which are required to intgrate TR graphs in BART, are set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd bart\n",
    "\n",
    "# Switch to desired branch of the BART project\n",
    "BRANCH=master\n",
    "git checkout $BRANCH\n",
    "\n",
    "# Define specifications \n",
    "COMPILE_SPECS=\" PARALLEL=1\n",
    "                TENSORFLOW=1\n",
    "                TENSORFLOW_BASE=../tensorflow/\"\n",
    "\n",
    "printf \"%s\\n\" $COMPILE_SPECS > Makefiles/Makefile.local\n",
    "\n",
    "make &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compilation of BART we need to set the required environmental variable: `TOOLBOX_PATH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TOOLBOX_PATH=/content/bart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we add the compiled `bart` executable to our `PATH` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RI9l6blElDF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] = os.environ['TOOLBOX_PATH'] + \":\" + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "743c3f4a"
   },
   "source": [
    "### Step 2: Help Information for TF Graph in BART's `pics`\n",
    "\n",
    "In the second step we have a look into the help for BART's regularization options for the `pics` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1f613571",
    "outputId": "6429ff8b-7993-4dd9-c841-dbb152c5c843",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bart pics -Rh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd4b2e2a"
   },
   "source": [
    "proximal operation on $R(x)$\n",
    "\n",
    "$$\\hat{x}=\\underset{x}{\\arg \\min} \\|x-v\\|^2 + \\lambda R(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e14c927"
   },
   "source": [
    "### Step 3: Extract Radial Spokes and Compute Coil Sensitivities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we downloaded above provides us with radial k-space data consisting of 160 spokes following a sampling scheme rotated by the 7th golden angle.\n",
    "\n",
    "For this tutorial we will use the first 60 spokes and extract them from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7942ccc3"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Extract spokes from original dataset\n",
    "\n",
    "spokes=60\n",
    "\n",
    "bart extract 2 0 $spokes ksp_256 ksp_256_c\n",
    "bart extract 2 0 $spokes traj_256 traj_256_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to exploit ESPIRiT for coil sensitivity estimation, we need to grid the non-Cartesian (radial) dataset. Instead of gridding it directly we use the internal gridding of the inverse `nufft` and project the result back into k-space with an `fft`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ca46fd4"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Grid non-Cartesian k-space data\n",
    "\n",
    "bart nufft -i traj_256_c ksp_256_c zero_filled\n",
    "bart fft $(bart bitmask 0 1) zero_filled grid_ksp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After gridding the radial dataset, we can use ESPIRiT to estimate the coil sensitivity maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Estimate coil-sensitivities with ESPIRiT\n",
    "\n",
    "bart ecalib -r20 -m1 -c0.0001 grid_ksp coilsen_esp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fcd3a99"
   },
   "source": [
    "## Example 1: $R(x)=\\|x\\|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the tf graph as an l2 regularization term, and compare with built-in l2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "334b8029"
   },
   "outputs": [],
   "source": [
    "!bart pics -i100 -R TF:{$(pwd)/l2_toy}:0.02 -d5 -e -t traj_256_c ksp_256_c coilsen_esp l2_pics_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4d9f2471"
   },
   "outputs": [],
   "source": [
    "!bart pics -l2 0.01 -e -d5 -t traj_256_c ksp_256_c coilsen_esp l2_pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "029c7626"
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axis = plt.subplots(figsize=(8,4), ncols=2)\n",
    "l2_pics = readcfl(\"l2_pics\")\n",
    "l2_pics_tf = readcfl(\"l2_pics_tf\")\n",
    "\n",
    "axis[0].imshow(abs(l2_pics), cmap='gray', interpolation='None')\n",
    "axis[1].imshow(abs(l2_pics_tf), cmap='gray', interpolation='None')\n",
    "axis[0].set_title(\"l2_pics\")\n",
    "axis[1].set_title(\"l2_pics_tf\")\n",
    "axis[0].axis('off')\n",
    "axis[1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da05b37c"
   },
   "source": [
    "## Example 2: $R(x)=\\log p(x, net(x))$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trained prior as a regularization term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a34231d2"
   },
   "outputs": [],
   "source": [
    "!ls prior/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edc18970"
   },
   "outputs": [],
   "source": [
    "# generate weights for density compensation\n",
    "writecfl(\"weights\", gen_weights(60, 256))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9b0b02b9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bart pics -i30 -R TF:{./prior/pixel_cnn}:8 -d5 -e -I -p weights -t traj_256_c ksp_256_c coilsen_esp w_pics_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c32249b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pics_prior = readcfl(\"w_pics_prior\")\n",
    "fig, axis = plt.subplots(figsize=(12,4), ncols=3)\n",
    "\n",
    "axis[0].imshow(abs(l2_pics), cmap='gray', interpolation='None')\n",
    "axis[0].set_title(\"l2_pics\")\n",
    "axis[1].imshow(abs(l2_pics_tf), cmap='gray', interpolation='None')\n",
    "axis[1].set_title(\"l2_pics_tf\")\n",
    "axis[2].imshow(abs(pics_prior), cmap='gray', interpolation='None')\n",
    "axis[2].set_title(\"prior_pics\")\n",
    "axis[0].axis('off')\n",
    "axis[1].axis('off')\n",
    "axis[2].axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2f804e22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'l2_toy.*': No such file or directory\r\n",
      "rm: cannot remove 'l2_toy_gpu_id': No such file or directory\r\n",
      "rm: cannot remove 'checkpoint': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! bash clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bart_tf.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
